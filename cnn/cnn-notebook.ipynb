{"cells":[{"cell_type":"markdown","metadata":{"id":"QYuALZOG-AMq"},"source":["## Assignment: Image recognition\n","- Alumno 1:\n","- Alumno 2:\n","- Alumno 3:\n","\n","The goals of the assignment are:\n","* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n","* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem.\n","* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task.\n","* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning.\n","\n","Follow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/4oNHlRFEd71HXp4](https://drive.upm.es/s/4oNHlRFEd71HXp4)"]},{"cell_type":"code","execution_count":21,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:29:47.888598884Z","start_time":"2023-10-06T13:29:47.733898014Z"},"execution":{"iopub.execute_input":"2024-03-22T09:26:37.237374Z","iopub.status.busy":"2024-03-22T09:26:37.237078Z","iopub.status.idle":"2024-03-22T09:26:37.249222Z","shell.execute_reply":"2024-03-22T09:26:37.248447Z","shell.execute_reply.started":"2024-03-22T09:26:37.237350Z"},"id":"OYtqD3Oh-AMw","trusted":true},"outputs":[],"source":["import uuid\n","import numpy as np\n","\n","class GenericObject:\n","    \"\"\"\n","    Generic object data.\n","    \"\"\"\n","    def __init__(self):\n","        self.id = uuid.uuid4()\n","        self.bb = (-1, -1, -1, -1)\n","        self.category= -1\n","        self.score = -1\n","\n","class GenericImage:\n","    \"\"\"\n","    Generic image data.\n","    \"\"\"\n","    def __init__(self, filename):\n","        self.filename = filename\n","        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n","        self.objects = list([])\n","\n","    def add_object(self, obj: GenericObject):\n","        self.objects.append(obj)"]},{"cell_type":"code","execution_count":22,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:29:47.895696607Z","start_time":"2023-10-06T13:29:47.798609562Z"},"execution":{"iopub.execute_input":"2024-03-22T09:26:37.250635Z","iopub.status.busy":"2024-03-22T09:26:37.250362Z","iopub.status.idle":"2024-03-22T09:26:37.264067Z","shell.execute_reply":"2024-03-22T09:26:37.263222Z","shell.execute_reply.started":"2024-03-22T09:26:37.250612Z"},"id":"I_GygShu-AMz","trusted":true},"outputs":[],"source":["categories = {0: 'Cargo plane', 1: 'Helicopter', 2: 'Small car', 3: 'Bus', 4: 'Truck', 5: 'Motorboat', 6: 'Fishing vessel', 7: 'Dump truck', 8: 'Excavator', 9: 'Building', 10: 'Storage tank', 11: 'Shipping container'}"]},{"cell_type":"code","execution_count":23,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:29:48.047107757Z","start_time":"2023-10-06T13:29:47.899181226Z"},"execution":{"iopub.execute_input":"2024-03-22T09:26:37.265535Z","iopub.status.busy":"2024-03-22T09:26:37.265224Z","iopub.status.idle":"2024-03-22T09:26:37.275837Z","shell.execute_reply":"2024-03-22T09:26:37.274946Z","shell.execute_reply.started":"2024-03-22T09:26:37.265511Z"},"id":"fRBA7ReQ-AM0","trusted":true},"outputs":[],"source":["import warnings\n","import rasterio\n","\n","def load_geoimage(filename):\n","    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n","    src_raster = rasterio.open(filename, 'r')\n","    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n","    input_type = src_raster.profile['dtype']\n","    input_channels = src_raster.count\n","    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n","    for band in range(input_channels):\n","        img[:, :, band] = src_raster.read(band+1)\n","    return img\n","\n","def generator_images(objs, batch_size, do_shuffle=False):\n","    while True:\n","        if do_shuffle:\n","            np.random.shuffle(objs)\n","        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n","        for group in groups:\n","            images, labels = [], []\n","            for (filename, obj) in group:\n","                # Load image\n","                images.append(load_geoimage(filename))\n","                probabilities = np.zeros(len(categories))\n","                probabilities[list(categories.values()).index(obj.category)] = 1\n","                labels.append(probabilities)\n","            images = np.array(images).astype(np.float32)\n","            labels = np.array(labels).astype(np.float32)\n","            yield images, labels"]},{"cell_type":"code","execution_count":24,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:29:49.392405670Z","start_time":"2023-10-06T13:29:49.059406441Z"},"execution":{"iopub.execute_input":"2024-03-22T09:26:37.277986Z","iopub.status.busy":"2024-03-22T09:26:37.277703Z","iopub.status.idle":"2024-03-22T09:26:37.291703Z","shell.execute_reply":"2024-03-22T09:26:37.290848Z","shell.execute_reply.started":"2024-03-22T09:26:37.277962Z"},"id":"HAanJ-V0-AM1","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def draw_confusion_matrix(cm, categories):\n","    # Draw confusion matrix\n","    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n","    ax = fig.add_subplot(111)\n","    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n","    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n","    ax.figure.colorbar(im, ax=ax)\n","    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n","    # Rotate the tick labels and set their alignment\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","    # Loop over data dimensions and create text annotations\n","    thresh = cm.max() / 2.0\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n","    fig.tight_layout()\n","    plt.show(fig)"]},{"cell_type":"markdown","metadata":{"id":"diNBB3qy-AM2"},"source":["#### Training\n","Design and train a ffNN to deal with the “xview_recognition” classification task."]},{"cell_type":"code","execution_count":25,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:29:50.354677859Z","start_time":"2023-10-06T13:29:50.239195303Z"},"execution":{"iopub.execute_input":"2024-03-22T09:26:37.293514Z","iopub.status.busy":"2024-03-22T09:26:37.293062Z","iopub.status.idle":"2024-03-22T09:26:37.409922Z","shell.execute_reply":"2024-03-22T09:26:37.409099Z","shell.execute_reply.started":"2024-03-22T09:26:37.293482Z"},"id":"Orto292C-AM3","trusted":true},"outputs":[],"source":["import json\n","\n","# Load database\n","json_file = '../dataset/xview_ann_train.json'\n","with open(json_file) as ifs:\n","    json_data = json.load(ifs)\n","ifs.close()"]},{"cell_type":"code","execution_count":26,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:29:51.688578027Z","start_time":"2023-10-06T13:29:51.386978797Z"},"execution":{"iopub.execute_input":"2024-03-22T09:26:37.412752Z","iopub.status.busy":"2024-03-22T09:26:37.412284Z","iopub.status.idle":"2024-03-22T09:26:38.085999Z","shell.execute_reply":"2024-03-22T09:26:38.085013Z","shell.execute_reply.started":"2024-03-22T09:26:37.412719Z"},"id":"4GjFLHs4-AM4","outputId":"5581df22-d4e9-42ac-9f94-061fd8c7acd9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Cargo plane': 635, 'Helicopter': 70, 'Small car': 4290, 'Bus': 2155, 'Truck': 2746, 'Motorboat': 1069, 'Fishing vessel': 706, 'Dump truck': 1236, 'Excavator': 789, 'Building': 4689, 'Storage tank': 1469, 'Shipping container': 1523}\n"]}],"source":["counts = dict.fromkeys(categories.values(), 0)\n","anns = []\n","for json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n","    image = GenericImage(json_img['filename'])\n","    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n","    obj = GenericObject()\n","    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n","    obj.category = json_ann['category_id']\n","    # Resampling strategy to reduce training time\n","    counts[obj.category] += 1\n","    image.add_object(obj)\n","    anns.append(image)\n","print(counts)"]},{"cell_type":"code","execution_count":29,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:29:56.123227463Z","start_time":"2023-10-06T13:29:53.523949443Z"},"execution":{"iopub.execute_input":"2024-03-22T09:29:22.634772Z","iopub.status.busy":"2024-03-22T09:29:22.634133Z","iopub.status.idle":"2024-03-22T09:29:37.076171Z","shell.execute_reply":"2024-03-22T09:29:37.074750Z","shell.execute_reply.started":"2024-03-22T09:29:22.634740Z"},"id":"BNkjbY2e-AM7","outputId":"47bde031-306f-464e-8e22-cc70a7fb7c67","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 17106 images belonging to 12 classes.\n","Found 4271 images belonging to 12 classes.\n"]},{"ename":"ResourceExhaustedError","evalue":"{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[6083072,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2] name: ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(n_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/models/sequential.py:117\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer, rebuild)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/models/sequential.py:136\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    135\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:224\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(original_build_method)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(obj\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39mobj):\n\u001b[0;32m--> 224\u001b[0m         \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/models/sequential.py:177\u001b[0m, in \u001b[0;36mSequential.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[6083072,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2] name: "]}],"source":["# Load architecture\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","batch_size = 64\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=40,\n","    validation_split=0.1\n",")\n","\n","train_generator = train_datagen.flow_from_directory(\n","    '../dataset/xview_train',\n","    target_size=(224, 224),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='training'\n",")\n","\n","valid_generator = train_datagen.flow_from_directory(\n","    '../dataset/xview_train',\n","    target_size=(224, 224),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='validation'\n",")\n","\n","model = Sequential()\n","model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=(224, 224, 3)))\n","model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\n","model.add(Flatten())\n","model.add(Dropout(0.5))\n","model.add(Dense(256, activation='relu'))\n","model.add(Dense(12, activation='softmax'))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:29:56.124072624Z","start_time":"2023-10-06T13:29:56.098552032Z"},"execution":{"iopub.status.busy":"2024-03-22T09:26:52.716753Z","iopub.status.idle":"2024-03-22T09:26:52.717115Z","shell.execute_reply":"2024-03-22T09:26:52.716953Z","shell.execute_reply.started":"2024-03-22T09:26:52.716937Z"},"id":"-aSlKtG6-AM7","trusted":true},"outputs":[],"source":["from tensorflow.keras.optimizers import Nadam\n","\n","opt = Nadam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, clipnorm=1.0, clipvalue=0.5)\n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:30:00.423165893Z","start_time":"2023-10-06T13:30:00.403263469Z"},"execution":{"iopub.status.busy":"2024-03-22T09:26:52.718323Z","iopub.status.idle":"2024-03-22T09:26:52.718658Z","shell.execute_reply":"2024-03-22T09:26:52.718512Z","shell.execute_reply.started":"2024-03-22T09:26:52.718497Z"},"id":"GGAJEfpB-AM8","trusted":true},"outputs":[],"source":["from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","\n","# Callbacks\n","model_checkpoint = ModelCheckpoint('model.keras', monitor='val_accuracy', verbose=1, save_best_only=True)\n","reduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=8, verbose=1)\n","early_stop = EarlyStopping('val_loss', patience=8, verbose=1)\n","terminate = TerminateOnNaN()\n","callbacks = [model_checkpoint, reduce_lr, early_stop, terminate]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:30:01.960819288Z","start_time":"2023-10-06T13:30:01.906083581Z"},"execution":{"iopub.status.busy":"2024-03-22T09:26:52.719822Z","iopub.status.idle":"2024-03-22T09:26:52.720268Z","shell.execute_reply":"2024-03-22T09:26:52.720064Z","shell.execute_reply.started":"2024-03-22T09:26:52.720045Z"},"id":"Yht-QqUH-AM8","trusted":true},"outputs":[],"source":["# Generate the list of objects from annotations\n","objs_train = [('../dataset/' + ann.filename, obj) for ann in train_generator for obj in ann.objects]\n","objs_valid = [('../dataset/' + ann.filename, obj) for ann in valid_generator for obj in ann.objects]\n","# Generators\n","#train_generator = generator_images(objs_train, batch_size, do_shuffle=True)\n","#valid_generator = generator_images(objs_valid, batch_size, do_shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T13:58:26.219192607Z","start_time":"2023-10-06T13:30:03.359181716Z"},"execution":{"iopub.status.busy":"2024-03-22T09:26:52.721415Z","iopub.status.idle":"2024-03-22T09:26:52.721847Z","shell.execute_reply":"2024-03-22T09:26:52.721647Z","shell.execute_reply.started":"2024-03-22T09:26:52.721628Z"},"id":"TrfpdECs-AM9","outputId":"21d89b78-d94c-442e-9bc2-517654c0b614","trusted":true},"outputs":[],"source":["import math\n","\n","print('Training model')\n","epochs = 20\n","train_steps = math.ceil(len(objs_train)/batch_size)\n","valid_steps = math.ceil(len(objs_valid)/batch_size)\n","\n","with tf.device(\"/device:GPU:0\"):\n","    h = model.fit(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks, verbose=1)\n","    # Best validation model\n","    best_idx = int(np.argmax(h.history['val_accuracy']))\n","    best_value = np.max(h.history['val_accuracy'])\n","    print('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))"]},{"cell_type":"markdown","metadata":{"id":"8IMMO_mT-AM9"},"source":["#### Testing\n","Try to improve the results provided in the Moodle competition wiki."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T14:06:30.286976834Z","start_time":"2023-10-06T14:06:30.243469253Z"},"execution":{"iopub.status.busy":"2024-03-22T09:26:52.722985Z","iopub.status.idle":"2024-03-22T09:26:52.723320Z","shell.execute_reply":"2024-03-22T09:26:52.723157Z","shell.execute_reply.started":"2024-03-22T09:26:52.723144Z"},"id":"Sgh9KqIW-AM-","trusted":true},"outputs":[],"source":["import json\n","\n","# Load database\n","json_file = '../dataset/xview_ann_test.json'\n","with open(json_file) as ifs:\n","    json_data = json.load(ifs)\n","ifs.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T14:06:31.133707360Z","start_time":"2023-10-06T14:06:31.083694789Z"},"execution":{"iopub.status.busy":"2024-03-22T09:26:52.724737Z","iopub.status.idle":"2024-03-22T09:26:52.725047Z","shell.execute_reply":"2024-03-22T09:26:52.724907Z","shell.execute_reply.started":"2024-03-22T09:26:52.724894Z"},"id":"tJr_-xCt-AM-","trusted":true},"outputs":[],"source":["anns = []\n","for json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n","    image = GenericImage(json_img['filename'])\n","    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n","    obj = GenericObject()\n","    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n","    obj.category = json_ann['category_id']\n","    image.add_object(obj)\n","    anns.append(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T14:08:35.738057751Z","start_time":"2023-10-06T14:06:31.815602308Z"},"execution":{"iopub.status.busy":"2024-03-22T09:26:52.726493Z","iopub.status.idle":"2024-03-22T09:26:52.726815Z","shell.execute_reply":"2024-03-22T09:26:52.726671Z","shell.execute_reply.started":"2024-03-22T09:26:52.726657Z"},"id":"TGs2zqfv-AM_","trusted":true},"outputs":[],"source":["# model.load_weights('model.hdf5', by_name=True)\n","y_true, y_pred = [], []\n","for ann in anns:\n","    # Load image\n","    image = load_geoimage('../dataset/' + ann.filename)\n","    for obj_pred in ann.objects:\n","        # Generate prediction\n","        warped_image = np.expand_dims(image, 0)\n","        predictions = model.predict(warped_image)\n","        # Save prediction\n","        pred_category = list(categories.values())[np.argmax(predictions)]\n","        pred_score = np.max(predictions)\n","        y_true.append(obj_pred.category)\n","        y_pred.append(pred_category)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T14:08:36.681807803Z","start_time":"2023-10-06T14:08:35.715011537Z"},"execution":{"iopub.status.busy":"2024-03-22T09:26:52.728037Z","iopub.status.idle":"2024-03-22T09:26:52.728514Z","shell.execute_reply":"2024-03-22T09:26:52.728268Z","shell.execute_reply.started":"2024-03-22T09:26:52.728249Z"},"id":"YqYKVsEp-AM_","outputId":"d256367d-744e-487d-f44c-cd106dad3484","trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","# Compute the confusion matrix\n","cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\n","draw_confusion_matrix(cm, categories)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-10-06T14:08:36.690503230Z","start_time":"2023-10-06T14:08:36.685130731Z"},"execution":{"iopub.status.busy":"2024-03-22T09:26:52.729805Z","iopub.status.idle":"2024-03-22T09:26:52.730241Z","shell.execute_reply":"2024-03-22T09:26:52.730037Z","shell.execute_reply.started":"2024-03-22T09:26:52.730019Z"},"id":"jD1zLfCd-ANA","outputId":"3a3154f8-c349-4a58-a129-d7aa04a588cb","trusted":true},"outputs":[],"source":["# Compute the accuracy\n","correct_samples_class = np.diag(cm).astype(float)\n","total_samples_class = np.sum(cm, axis=1).astype(float)\n","total_predicts_class = np.sum(cm, axis=0).astype(float)\n","print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n","acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n","print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n","acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n","print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n","for idx in range(len(categories)):\n","    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n","    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n","    tp = cm[idx, idx]\n","    fp = sum(cm[:, idx]) - tp\n","    fn = sum(cm[idx, :]) - tp\n","    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n","    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n","    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n","    # Precision: proportion of predicted positive cases that were truly real positives.\n","    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n","    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n","    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n","    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n","    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n","    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n","    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))"]},{"cell_type":"markdown","metadata":{"id":"uOPI5zc8-ANB"},"source":["#### Report\n","\n","You must prepare a report (PDF) describing:\n","* The problems and data sets (briefly).\n","* The process that you have followed to reach your solution for the “xview_recognition” benchmark, including your intermediate results. You must discuss and compare these results properly.\n","* Final network architectures, including optimization algorithms, regularization methods (dropout, data augmentation, etc.), number of layers/parameters, and performance obtained with your model on the train/valid/test data sets, including the plots of the evolution of losses and accuracy.\n","* It would also be very valuable your feedback on the use of “Cesvima” or “Google Colab\" services.\n","\n","In the submission via Moodle, attach your Python (.py) or Jupyter Notebook (.ipynb) source file, including in the report all results of computations attached to the code that generated them.\n","\n","The assignment must be done in groups of 3 students."]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4537670,"sourceId":7759263,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
